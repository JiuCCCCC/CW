@article{askell2021language,
  title={A General Language Assistant as a Laboratory for Alignment},
  author={Askell, Amanda and Bai, Yuntao and ...},
  journal={arXiv preprint},
  year={2021},
  publisher={arXiv},
  institution={Anthropic},
  doi={10.48550/arXiv.2112.00861},
  url={https://arxiv.org/abs/2112.00861},
  keywords={type:alignment,data:alignment, language models, reinforcement learning, human feedback},
  type={alignment},
  series={alignment foundations},
  abstract={This paper proposes using a general language assistant as a laboratory for studying AI alignment approaches...},
  concept={Using a general-purpose language assistant to study AI alignment.},
  method={Deploying LMs as alignment labs; RLHF-based techniques.},
  related_work={Prior RLHF and AI alignment frameworks.},
  input_data={Language-based interactive datasets with human feedback.},
  evaluation={Empirical qualitative analysis on alignment tasks.},
  representative_image={askell2021language.png},
  pdf_url={https://arxiv.org/pdf/2112.00861.pdf}
}

@article{bai2022constitutional,
  title={Constitutional AI: Harmlessness from AI Feedback},
  author={Bai, Yuntao and Kadavath, Saurav and ...},
  journal={arXiv preprint},
  year={2022},
  publisher={arXiv},
  institution={Anthropic and Google},
  doi={10.48550/arXiv.2212.08073},
  url={https://arxiv.org/abs/2212.08073},
  keywords={type:alignment,data:constitutional AI, harmlessness, AI feedback, alignment},
  type={alignment},
  series={training methods},
  abstract={We introduce Constitutional AI (CAI), a method for training AI systems that are helpful, harmless, and honest...},
  concept={Proposes Constitutional AI to replace human feedback with AI-generated feedback.},
  method={Training guided by natural-language rules (the “constitution”).},
  related_work={RLHF and other alignment training methods.},
  input_data={Prompt-response pairs evaluated against rules.},
  evaluation={Behavioral comparisons, human evaluations.},
  representative_image={bai2022constitutional.png},
  pdf_url={https://arxiv.org/pdf/2212.08073.pdf}
}

@article{ganguli2022red,
  title={Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned},
  author={Ganguli, Deep and Lovitt, Liane and ...},
  journal={arXiv preprint},
  year={2022},
  publisher={arXiv},
  institution={Anthropic},
  doi={10.48550/arXiv.2209.07858},
  url={https://arxiv.org/abs/2209.07858},
  keywords={type:safety-evaluation,data:red teaming, adversarial testing, safety evaluation, harm reduction},
  type={safety-evaluation},
  series={evaluation and vulnerability discovery},
  abstract={We investigate using red teaming to reduce harms in language models at scale through systematic vulnerability testing...},
  concept={Red teaming language models to uncover and mitigate harms.},
  method={Systematic adversarial testing of LMs with red team strategies.},
  related_work={Harm mitigation in AI, safety strategies.},
  input_data={Varied tasks for adversarial inputs.},
  evaluation={Scalability and effectiveness analysis of red teaming.},
  representative_image={ganguli2022red.png},
  pdf_url={https://arxiv.org/pdf/2209.07858.pdf}
}

@article{li2024scisafeeval,
  title={SciSafeEval: A Comprehensive Benchmark for Safety Alignment of Large Language Models in Scientific Tasks},
  author={Li, Tianhao and Lu, Jingyu and Chu, Chuangxin and Zeng, Tianyu and Zheng, Yujia and Li, Mei and Huang, Haotian and Wu, Bin and Liu, Zuoxian and Ma, Kai and Yuan, Xuejing and Wang, Xingkai and Ding, Keyan and Chen, Huajun and Zhang, Qiang},
  journal={arXiv preprint},
  year={2024},
  publisher={arXiv},
  doi={10.48550/arXiv.2410.03769},
  url={https://arxiv.org/abs/2410.03769},
  keywords={type:safety-evaluation, data:scientific tasks, safety alignment, jailbreak testing, multi-modal evaluation},
  type={safety-evaluation},
  series={LLM safety evaluation in scientific domains},
  abstract={This study introduces SciSafeEval, a comprehensive benchmark designed to evaluate the safety alignment of LLMs across various scientific tasks, including textual, molecular, protein, and genomic languages, incorporating 'jailbreak' enhancements to test model defenses.},
  concept={Assessing LLM safety alignment in diverse scientific contexts.},
  method={Benchmarking across zero-shot, few-shot, and chain-of-thought settings with jailbreak scenarios.},
  related_work={Safety benchmarks in NLP, multi-modal evaluation datasets.},
  input_data={Scientific texts, molecular structures, protein sequences, genomic data.},
  evaluation={Model performance on safety and task-specific metrics across scientific domains.},
  representative_image={li2024scisafeeval.png},
  pdf_url={https://arxiv.org/pdf/2410.03769.pdf}
}

@article{greshake2023prompt,
  title={Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Prompt Injection},
  author={Greshake, Kai and Liu, Sahar and ...},
  journal={arXiv preprint},
  year={2023},
  publisher={arXiv},
  doi={10.48550/arXiv.2302.12173},
  url={https://arxiv.org/abs/2302.12173},
  keywords={type:attack-analysis,data:security vulnerabilities, application security, defense mechanisms},
  type={attack-analysis},
  series={attack surface and threat model},
  abstract={We demonstrate how prompt injection attacks can compromise real-world LLM applications...},
  concept={Prompt injection threats in deployed LLM applications.},
  method={Security experiments exploiting prompts.},
  related_work={Security in NLP, prompt-based threats.},
  input_data={Real-world LLM-based app interfaces.},
  evaluation={Attack success rate and system compromise analysis.},
  representative_image={greshake2023prompt.png},
  pdf_url={https://arxiv.org/pdf/2302.12173.pdf}
}

@article{guan2023deliberative,
  title={Deliberative Alignment: Reasoning Enables Safer Language Models},
  author={Guan, Melody Y. and Askell, Amanda and Krueger, David and McAleese, Natasha Jaques and Saunders, William and Bowman, Samuel R. and Ganguli, Deep and Christiano, Paul and Irving, Geoffrey and Amodei, Dario},
  journal={arXiv preprint},
  year={2023},
  publisher={arXiv},
  doi={10.48550/arXiv.2312.16339},
  url={https://arxiv.org/abs/2412.16339},
  keywords={type:alignment, data:safety evaluation, reasoning, deliberation, safety alignment},
  type={alignment},
  series={safety and alignment strategies},
  abstract={This paper proposes Deliberative Alignment, a framework for training LLMs to reason about alignment before answering questions, resulting in safer and more controllable outputs...},
  concept={Using deliberative reasoning to enhance LLM safety and alignment.},
  method={Chain-of-thought prompting for internal deliberation before answer generation.},
  related_work={AI alignment strategies, safety alignment in LLMs, chain-of-thought reasoning.},
  input_data={Aligned question-answering datasets and safety scenarios.},
  evaluation={Safety benchmarks and alignment accuracy metrics.},
  representative_image={guan2023deliberative.png},
  pdf_url={https://arxiv.org/pdf/2412.16339}
}


@article{ji2025inferenceguard,
  title={Almost Surely Safe Alignment of Large Language Models at Inference-Time},
  author={Ji, Xiaotong and Ramesh, Shyam Sundhar and Zimmer, Matthieu and Bogunovic, Ilija and Wang, Jun and Bou Ammar, Haitham},
  journal={arXiv preprint},
  year={2025},
  publisher={arXiv},
  doi={10.48550/arXiv.2502.01208},
  url={https://arxiv.org/abs/2502.01208},
  keywords={type:inference-safety, data:safety constraints, constrained MDP, latent space, safety guarantees},
  type={inference-safety},
  series={safety mechanisms in LLMs},
  abstract={This paper introduces InferenceGuard, a novel inference-time alignment approach that ensures LLMs generate safe responses almost surely by framing response generation as a constrained Markov decision process within the LLM's latent space.},
  concept={Implementing safety constraints during inference without modifying model weights.},
  method={Constrained Markov decision process in latent space with safety state tracking.},
  related_work={RLHF, inference-time alignment methods.},
  input_data={LLM latent representations during inference.},
  evaluation={Safety and task performance metrics compared to existing methods.},
  representative_image={ji2025inferenceguard.png},
  pdf_url={https://arxiv.org/pdf/2502.01208.pdf}
}


@article{li2024safetylayers,
  title={Safety Layers in Aligned Large Language Models: The Key to LLM Security},
  author={Li, Shen and Yao, Liuyi and Zhang, Lan and Li, Yaliang},
  journal={arXiv preprint},
  year={2024},
  publisher={arXiv},
  doi={10.48550/arXiv.2408.17003},
  url={https://arxiv.org/abs/2408.17003},
  keywords={type:inference-safety, data:safety layers, SPPFT, fine-tuning defense, model security},
  type={inference-safety},
  series={internal mechanisms of LLM safety},
  abstract={This paper uncovers the existence of 'safety layers' within aligned LLMs that are crucial for distinguishing malicious queries, and proposes Safely Partial-Parameter Fine-Tuning (SPPFT) to preserve security during fine-tuning.},
  concept={Identifying and preserving critical layers responsible for LLM security.},
  method={Layer-wise analysis and gradient freezing during fine-tuning (SPPFT).},
  related_work={LLM fine-tuning methods, model robustness studies.},
  input_data={Aligned LLMs subjected to fine-tuning with various datasets.},
  evaluation={Security retention and performance metrics post fine-tuning.},
  representative_image={li2024safetylayers.png},
  pdf_url={https://arxiv.org/pdf/2408.17003.pdf}
}


@article{pan2025hiddendimensions,
  title={The Hidden Dimensions of LLM Alignment: A Multi-Dimensional Safety Analysis},
  author={Pan, Wenbo and Liu, Zhichao and Chen, Qiguang and Zhou, Xiangyang and Yu, Haining and Jia, Xiaohua},
  journal={arXiv preprint},
  year={2025},
  publisher={arXiv},
  doi={10.48550/arXiv.2502.09674},
  url={https://arxiv.org/abs/2502.09674},
  keywords={type:inference-safety, data:multi-dimensional safety vectors, jailbreak vulnerability},
  type={inference-safety},
  series={mechanistic interpretability in LLMs},
  abstract={This paper reveals that safety-aligned behaviors in LLMs are governed by multiple orthogonal directions in activation space, with secondary directions influencing model refusal behavior and potential vulnerabilities.},
  concept={Multi-dimensional representation of safety alignment in LLMs.},
  method={Vector space analysis of representation shifts during safety fine-tuning.},
  related_work={Mechanistic interpretability, safety alignment.},
  input_data={Llama 3 8B model with jailbreak prompts.},
  evaluation={Impact of secondary directions on model refusal behavior.},
  representative_image={pan2025hiddendimensions.png},
  pdf_url={https://arxiv.org/pdf/2502.09674.pdf}
}


@article{zhao2024acd,
  title={Adversarial Contrastive Decoding: Boosting Safety Alignment of Large Language Models via Opposite Prompt Optimization},
  author={Zhao Zhengyue , Zhang Xiaoyun, Xu Kaidi,  Hu Xing,Zhang Rui, Du Zidong, Guo Qi, Chen, Yunji},
  journal={arXiv preprint},
  year={2024},
  publisher={arXiv},
  doi={10.48550/arXiv.2406.16743},
  url={https://arxiv.org/abs/2406.16743},
  keywords={type:inference-safety, data:prompt pairing, safety decoding},
  type={inference-safety},
  series={adversarial prompt safety},
  abstract={This paper proposes Adversarial Contrastive Decoding (ACD), which optimizes safety alignment by decoding from oppositely optimized prompts without altering LLM weights.},
  concept={Improving safety via contrast between positive and adversarial prompts.},
  method={Prompt-level contrastive decoding with anchor-tuning.},
  related_work={Prompt tuning for safety, adversarial prompt mitigation.},
  input_data={Safety-critical prompts with contrastive pairs.},
  evaluation={Safety rate, utility, and prompt generalization.},
  representative_image={zhao2024acd.png},
  pdf_url={https://arxiv.org/pdf/2406.16743.pdf}
}


