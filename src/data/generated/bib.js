const generatedBibEntries = {
    "askell2021language": {
        "abstract": "This paper proposes using a general language assistant as a laboratory for studying AI alignment approaches...",
        "author": "Askell, Amanda and Bai, Yuntao and ...",
        "concept": "Using a general-purpose language assistant to study AI alignment.",
        "doi": "10.48550/arXiv.2112.00861",
        "evaluation": "Empirical qualitative analysis on alignment tasks.",
        "input_data": "Language-based interactive datasets with human feedback.",
        "institution": "Anthropic",
        "journal": "arXiv preprint",
        "keywords": "type:alignment,data:alignment, language models, reinforcement learning, human feedback",
        "method": "Deploying LMs as alignment labs; RLHF-based techniques.",
        "pdf_url": "https://arxiv.org/pdf/2112.00861.pdf",
        "publisher": "arXiv",
        "related_work": "Prior RLHF and AI alignment frameworks.",
        "representative_image": "askell2021language.png",
        "series": "alignment foundations",
        "title": "A General Language Assistant as a Laboratory for Alignment",
        "type": "article alignment",
        "url": "https://arxiv.org/abs/2112.00861",
        "year": "2021"
    },
    "bai2022constitutional": {
        "abstract": "We introduce Constitutional AI (CAI), a method for training AI systems that are helpful, harmless, and honest...",
        "author": "Bai, Yuntao and Kadavath, Saurav and ...",
        "concept": "Proposes Constitutional AI to replace human feedback with AI-generated feedback.",
        "doi": "10.48550/arXiv.2212.08073",
        "evaluation": "Behavioral comparisons, human evaluations.",
        "input_data": "Prompt-response pairs evaluated against rules.",
        "institution": "Anthropic and Google",
        "journal": "arXiv preprint",
        "keywords": "type:alignment,data:constitutional AI, harmlessness, AI feedback, alignment",
        "method": "Training guided by natural-language rules (the \u201cconstitution\u201d).",
        "pdf_url": "https://arxiv.org/pdf/2212.08073.pdf",
        "publisher": "arXiv",
        "related_work": "RLHF and other alignment training methods.",
        "representative_image": "bai2022constitutional.png",
        "series": "training methods",
        "title": "Constitutional AI: Harmlessness from AI Feedback",
        "type": "article alignment",
        "url": "https://arxiv.org/abs/2212.08073",
        "year": "2022"
    },
    "ganguli2022red": {
        "abstract": "We investigate using red teaming to reduce harms in language models at scale through systematic vulnerability testing...",
        "author": "Ganguli, Deep and Lovitt, Liane and ...",
        "concept": "Red teaming language models to uncover and mitigate harms.",
        "doi": "10.48550/arXiv.2209.07858",
        "evaluation": "Scalability and effectiveness analysis of red teaming.",
        "input_data": "Varied tasks for adversarial inputs.",
        "institution": "Anthropic",
        "journal": "arXiv preprint",
        "keywords": "type:safety-evaluation,data:red teaming, adversarial testing, safety evaluation, harm reduction",
        "method": "Systematic adversarial testing of LMs with red team strategies.",
        "pdf_url": "https://arxiv.org/pdf/2209.07858.pdf",
        "publisher": "arXiv",
        "related_work": "Harm mitigation in AI, safety strategies.",
        "representative_image": "ganguli2022red.png",
        "series": "evaluation and vulnerability discovery",
        "title": "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned",
        "type": "article safety-evaluation",
        "url": "https://arxiv.org/abs/2209.07858",
        "year": "2022"
    },
    "greshake2023prompt": {
        "abstract": "We demonstrate how prompt injection attacks can compromise real-world LLM applications...",
        "author": "Greshake, Kai and Liu, Sahar and ...",
        "concept": "Prompt injection threats in deployed LLM applications.",
        "doi": "10.48550/arXiv.2302.12173",
        "evaluation": "Attack success rate and system compromise analysis.",
        "input_data": "Real-world LLM-based app interfaces.",
        "journal": "arXiv preprint",
        "keywords": "type:attack-analysis,data:security vulnerabilities, application security, defense mechanisms",
        "method": "Security experiments exploiting prompts.",
        "pdf_url": "https://arxiv.org/pdf/2302.12173.pdf",
        "publisher": "arXiv",
        "related_work": "Security in NLP, prompt-based threats.",
        "representative_image": "greshake2023prompt.png",
        "series": "attack surface and threat model",
        "title": "Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Prompt Injection",
        "type": "article attack-analysis",
        "url": "https://arxiv.org/abs/2302.12173",
        "year": "2023"
    },
    "guan2023deliberative": {
        "abstract": "This paper proposes Deliberative Alignment, a framework for training LLMs to reason about alignment before answering questions, resulting in safer and more controllable outputs...",
        "author": "Guan, Melody Y. and Askell, Amanda and Krueger, David and McAleese, Natasha Jaques and Saunders, William and Bowman, Samuel R. and Ganguli, Deep and Christiano, Paul and Irving, Geoffrey and Amodei, Dario",
        "concept": "Using deliberative reasoning to enhance LLM safety and alignment.",
        "doi": "10.48550/arXiv.2312.16339",
        "evaluation": "Safety benchmarks and alignment accuracy metrics.",
        "input_data": "Aligned question-answering datasets and safety scenarios.",
        "journal": "arXiv preprint",
        "keywords": "type:alignment, data:safety evaluation, reasoning, deliberation, safety alignment",
        "method": "Chain-of-thought prompting for internal deliberation before answer generation.",
        "pdf_url": "https://arxiv.org/pdf/2412.16339",
        "publisher": "arXiv",
        "related_work": "AI alignment strategies, safety alignment in LLMs, chain-of-thought reasoning.",
        "representative_image": "guan2023deliberative.png",
        "series": "safety and alignment strategies",
        "title": "Deliberative Alignment: Reasoning Enables Safer Language Models",
        "type": "article alignment",
        "url": "https://arxiv.org/abs/2412.16339",
        "year": "2023"
    },
    "ji2025inferenceguard": {
        "abstract": "This paper introduces InferenceGuard, a novel inference-time alignment approach that ensures LLMs generate safe responses almost surely by framing response generation as a constrained Markov decision process within the LLM's latent space.",
        "author": "Ji, Xiaotong and Ramesh, Shyam Sundhar and Zimmer, Matthieu and Bogunovic, Ilija and Wang, Jun and Bou Ammar, Haitham",
        "concept": "Implementing safety constraints during inference without modifying model weights.",
        "doi": "10.48550/arXiv.2502.01208",
        "evaluation": "Safety and task performance metrics compared to existing methods.",
        "input_data": "LLM latent representations during inference.",
        "journal": "arXiv preprint",
        "keywords": "type:inference-safety, data:safety constraints, constrained MDP, latent space, safety guarantees",
        "method": "Constrained Markov decision process in latent space with safety state tracking.",
        "pdf_url": "https://arxiv.org/pdf/2502.01208.pdf",
        "publisher": "arXiv",
        "related_work": "RLHF, inference-time alignment methods.",
        "representative_image": "ji2025inferenceguard.png",
        "series": "safety mechanisms in LLMs",
        "title": "Almost Surely Safe Alignment of Large Language Models at Inference-Time",
        "type": "article inference-safety",
        "url": "https://arxiv.org/abs/2502.01208",
        "year": "2025"
    },
    "li2024safetylayers": {
        "abstract": "This paper uncovers the existence of 'safety layers' within aligned LLMs that are crucial for distinguishing malicious queries, and proposes Safely Partial-Parameter Fine-Tuning (SPPFT) to preserve security during fine-tuning.",
        "author": "Li, Shen and Yao, Liuyi and Zhang, Lan and Li, Yaliang",
        "concept": "Identifying and preserving critical layers responsible for LLM security.",
        "doi": "10.48550/arXiv.2408.17003",
        "evaluation": "Security retention and performance metrics post fine-tuning.",
        "input_data": "Aligned LLMs subjected to fine-tuning with various datasets.",
        "journal": "arXiv preprint",
        "keywords": "type:inference-safety, data:safety layers, SPPFT, fine-tuning defense, model security",
        "method": "Layer-wise analysis and gradient freezing during fine-tuning (SPPFT).",
        "pdf_url": "https://arxiv.org/pdf/2408.17003.pdf",
        "publisher": "arXiv",
        "related_work": "LLM fine-tuning methods, model robustness studies.",
        "representative_image": "li2024safetylayers.png",
        "series": "internal mechanisms of LLM safety",
        "title": "Safety Layers in Aligned Large Language Models: The Key to LLM Security",
        "type": "article inference-safety",
        "url": "https://arxiv.org/abs/2408.17003",
        "year": "2024"
    },
    "li2024scisafeeval": {
        "abstract": "This study introduces SciSafeEval, a comprehensive benchmark designed to evaluate the safety alignment of LLMs across various scientific tasks, including textual, molecular, protein, and genomic languages, incorporating 'jailbreak' enhancements to test model defenses.",
        "author": "Li, Tianhao and Lu, Jingyu and Chu, Chuangxin and Zeng, Tianyu and Zheng, Yujia and Li, Mei and Huang, Haotian and Wu, Bin and Liu, Zuoxian and Ma, Kai and Yuan, Xuejing and Wang, Xingkai and Ding, Keyan and Chen, Huajun and Zhang, Qiang",
        "concept": "Assessing LLM safety alignment in diverse scientific contexts.",
        "doi": "10.48550/arXiv.2410.03769",
        "evaluation": "Model performance on safety and task-specific metrics across scientific domains.",
        "input_data": "Scientific texts, molecular structures, protein sequences, genomic data.",
        "journal": "arXiv preprint",
        "keywords": "type:safety-evaluation, data:scientific tasks, safety alignment, jailbreak testing, multi-modal evaluation",
        "method": "Benchmarking across zero-shot, few-shot, and chain-of-thought settings with jailbreak scenarios.",
        "pdf_url": "https://arxiv.org/pdf/2410.03769.pdf",
        "publisher": "arXiv",
        "related_work": "Safety benchmarks in NLP, multi-modal evaluation datasets.",
        "representative_image": "li2024scisafeeval.png",
        "series": "LLM safety evaluation in scientific domains",
        "title": "SciSafeEval: A Comprehensive Benchmark for Safety Alignment of Large Language Models in Scientific Tasks",
        "type": "article safety-evaluation",
        "url": "https://arxiv.org/abs/2410.03769",
        "year": "2024"
    },
    "pan2025hiddendimensions": {
        "abstract": "This paper reveals that safety-aligned behaviors in LLMs are governed by multiple orthogonal directions in activation space, with secondary directions influencing model refusal behavior and potential vulnerabilities.",
        "author": "Pan, Wenbo and Liu, Zhichao and Chen, Qiguang and Zhou, Xiangyang and Yu, Haining and Jia, Xiaohua",
        "concept": "Multi-dimensional representation of safety alignment in LLMs.",
        "doi": "10.48550/arXiv.2502.09674",
        "evaluation": "Impact of secondary directions on model refusal behavior.",
        "input_data": "Llama 3 8B model with jailbreak prompts.",
        "journal": "arXiv preprint",
        "keywords": "type:inference-safety, data:multi-dimensional safety vectors, jailbreak vulnerability",
        "method": "Vector space analysis of representation shifts during safety fine-tuning.",
        "pdf_url": "https://arxiv.org/pdf/2502.09674.pdf",
        "publisher": "arXiv",
        "related_work": "Mechanistic interpretability, safety alignment.",
        "representative_image": "pan2025hiddendimensions.png",
        "series": "mechanistic interpretability in LLMs",
        "title": "The Hidden Dimensions of LLM Alignment: A Multi-Dimensional Safety Analysis",
        "type": "article inference-safety",
        "url": "https://arxiv.org/abs/2502.09674",
        "year": "2025"
    },
    "zhao2024acd": {
        "abstract": "This paper proposes Adversarial Contrastive Decoding (ACD), which optimizes safety alignment by decoding from oppositely optimized prompts without altering LLM weights.",
        "author": "Zhao Zhengyue , Zhang Xiaoyun, Xu Kaidi,  Hu Xing,Zhang Rui, Du Zidong, Guo Qi, Chen, Yunji",
        "concept": "Improving safety via contrast between positive and adversarial prompts.",
        "doi": "10.48550/arXiv.2406.16743",
        "evaluation": "Safety rate, utility, and prompt generalization.",
        "input_data": "Safety-critical prompts with contrastive pairs.",
        "journal": "arXiv preprint",
        "keywords": "type:inference-safety, data:prompt pairing, safety decoding",
        "method": "Prompt-level contrastive decoding with anchor-tuning.",
        "pdf_url": "https://arxiv.org/pdf/2406.16743.pdf",
        "publisher": "arXiv",
        "related_work": "Prompt tuning for safety, adversarial prompt mitigation.",
        "representative_image": "zhao2024acd.png",
        "series": "adversarial prompt safety",
        "title": "Adversarial Contrastive Decoding: Boosting Safety Alignment of Large Language Models via Opposite Prompt Optimization",
        "type": "article inference-safety",
        "url": "https://arxiv.org/abs/2406.16743",
        "year": "2024"
    }
};